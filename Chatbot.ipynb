{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('popular', quiet=True)\n",
    "# nltk.download('punkt') # first-time use only\n",
    "# nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the corpus\n",
    "\n",
    "We will be using the Wikipedia page for chatbots as our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('wikipedia-first.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword matching\n",
    "\n",
    "Used for greet response by the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
    "GREETING_RESPONSES = [ \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Google's cloud language API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from nlpGoogleAPI import language_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi, I'm a WikiBot and i am here to answer your queries. If you want to exit, type Bye!\n",
      "Hello\n",
      "\n",
      "WikiBot: hey\n",
      "=============================================================\n",
      "Who is king arthur?\n",
      "\n",
      "WikiBot: king arthur\n",
      "king arthur is a legendary king in the mythology of great britain.\n",
      " \n",
      "<==== Here is some more info based on your query ====>\n",
      " \n",
      "Representative name identified for the entity: king arthur\n",
      "Entity type: PERSON\n",
      "\n",
      "*** *** *** *** *** ***\n",
      "Mention text: king arthur\n",
      "Mention type: COMMON\n",
      " \n",
      "Salience score: 1.0\n",
      " \n",
      "Sentiment: 0.0, 0.0\n",
      "=============================================================\n",
      "Is covid-19 a deadly disease?\n",
      "\n",
      "WikiBot: bubonic plague\n",
      "the bubonic plague is a very deadly disease.\n",
      " \n",
      "<==== Here is some more info based on your query ====>\n",
      " \n",
      "Representative name identified for the entity: disease\n",
      "Entity type: OTHER\n",
      "\n",
      "*** *** *** *** *** ***\n",
      "Mention text: disease\n",
      "Mention type: COMMON\n",
      " \n",
      "Salience score: 1.0\n",
      " \n",
      "Representative name identified for the entity: 19\n",
      "Entity type: NUMBER\n",
      "value: 19\n",
      "\n",
      "*** *** *** *** *** ***\n",
      "Mention text: 19\n",
      "Mention type: TYPE_UNKNOWN\n",
      " \n",
      "Salience score: 0.0\n",
      " \n",
      "Sentiment: -0.10000000149011612, 0.10000000149011612\n",
      "=============================================================\n",
      "How to go to seattle from Riverside?\n",
      "\n",
      "WikiBot: riverside, iowa\n",
      "riverside, iowa is a city of iowa in the united states.\n",
      " \n",
      "<==== Here is some more info based on your query ====>\n",
      " \n",
      "Representative name identified for the entity: seattle\n",
      "Entity type: LOCATION\n",
      "wikipedia_url: https://en.wikipedia.org/wiki/Seattle\n",
      "mid: /m/0d9jr\n",
      "\n",
      "*** *** *** *** *** ***\n",
      "Mention text: seattle\n",
      "Mention type: PROPER\n",
      " \n",
      "Salience score: 0.649021565914154\n",
      " \n",
      "Representative name identified for the entity: riverside\n",
      "Entity type: LOCATION\n",
      "\n",
      "*** *** *** *** *** ***\n",
      "Mention text: riverside\n",
      "Mention type: COMMON\n",
      " \n",
      "Salience score: 0.35097840428352356\n",
      " \n",
      "Sentiment: 0.0, 0.0\n",
      "Map directions : https://www.google.com/maps/dir/?api=1&origin=seattle&destination=riverside&travelmode=car\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"\")\n",
    "print(\"Hi, I'm a WikiBot and i am here to answer your queries. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"\")\n",
    "            print(\"WikiBot: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"\")\n",
    "                print(\"WikiBot: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"\")\n",
    "                print(\"WikiBot: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "                sentiment, entities = language_analysis(user_response)\n",
    "                #print(entities, sentiment)\n",
    "                if(len(entities)>0):\n",
    "                    print(\" \")\n",
    "                    print(\"<==== Here is some more info based on your query ====>\")\n",
    "\n",
    "                    print(\" \")\n",
    "    \n",
    "                    for e in entities:\n",
    "                        print(u\"Representative name identified for the entity: {}\".format(e.name))\n",
    "                        print(u\"Entity type: {}\".format(enums.Entity.Type(e.type).name))\n",
    "                        for metadata_name, metadata_value in e.metadata.items():\n",
    "                            print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "                        print(\"\")\n",
    "                                                    \n",
    "                        print(\"*** *** *** *** *** ***\")\n",
    "                        for mention in e.mentions:\n",
    "                            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "                            # Get the mention type, e.g. PROPER for proper noun\n",
    "                            print(u\"Mention type: {}\".format(enums.EntityMention.Type(mention.type).name))\n",
    "                        print(\" \")\n",
    "                        #print('Text: {}'.format(text))\n",
    "                        print(u\"Salience score: {}\".format(e.salience))\n",
    "                        print(\" \")\n",
    "                    print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))\n",
    "                    if(len(entities) >= 2 and enums.Entity.Type(entities[0].type).name == 'LOCATION' and enums.Entity.Type(entities[1].type).name == 'LOCATION'):\n",
    "                        print('Map directions : ' + 'https://www.google.com/maps/dir/?api=1&origin='+ entities[0].mentions[0].text.content +'&destination='+ entities[1].mentions[0].text.content + '&travelmode=car')\n",
    "                \n",
    "        print(\"=============================================================\")\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"\")\n",
    "        print(\"WikiBot: Bye! Have fun..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
